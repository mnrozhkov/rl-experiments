{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojSvCoXN6NTQ"
   },
   "source": [
    "## Установка пакетов:\n",
    "\n",
    "`pip install gym[atari]` -- непосредственно наша тестовая среда с различными играми\n",
    "\n",
    "`pip install tqdm` -- progress bar для python \n",
    "\n",
    "`pip install keras` -- библиотека глубинного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXvJxSfn6PKR",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (0.10.9)\n",
      "Requirement already satisfied: tqdm in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (4.28.1)\n",
      "Requirement already satisfied: keras in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from gym) (1.15.4)\n",
      "Requirement already satisfied: scipy in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from gym) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.0 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from gym) (2.20.0)\n",
      "Requirement already satisfied: pyyaml in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: h5py in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Collecting keras_applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
      "Collecting keras_preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: future in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from pyglet>=1.2.0->gym) (0.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (2018.10.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (2.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/mnrozhkov/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym) (1.23)\n",
      "Installing collected packages: keras-applications, keras-preprocessing\n",
      "  Found existing installation: Keras-Applications 1.0.4\n",
      "    Uninstalling Keras-Applications-1.0.4:\n",
      "      Successfully uninstalled Keras-Applications-1.0.4\n",
      "  Found existing installation: Keras-Preprocessing 1.0.2\n",
      "    Uninstalling Keras-Preprocessing-1.0.2:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.0.2\n",
      "Successfully installed keras-applications-1.0.6 keras-preprocessing-1.0.5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install gym gym[atari] tqdm keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYGFLBJS6NTS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, InputLayer\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import cv2\n",
    "import time\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JBq-gDi66NTY",
    "outputId": "2c2640a6-067e-4b90-daa8-4a4d925c060a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.4', '0.10.9')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(keras.__version__, gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5EMfGJF6NTc"
   },
   "source": [
    "## Знакомство с OpenAI Gym\n",
    "\n",
    "# TODO\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) -- это фреймворк с коллекцией разнообразных тестовых сред для обучения, наподобие набора данных ImageNet.\n",
    "\n",
    "Основная идея стоит в стандартизации тестовых сред для более легкого воспроизведения результатов научных публикаций.\n",
    "\n",
    "За основые среды для обучения наших моделек, а также для последующего соревнования возьмем среды игр Atari:\n",
    "\n",
    "* [Breakout](https://gym.openai.com/envs/Breakout-v0/)\n",
    "\n",
    "* [SpaceInvaders](https://gym.openai.com/envs/SpaceInvaders-v0)\n",
    "\n",
    "* [MsPacman](https://gym.openai.com/envs/MsPacman-v0/)\n",
    "\n",
    "Посмотрим на одну из игр подробнее. В силу стандартизированности тестовых сред, для изучения других игр вам понадобится изменить только название среды :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgWtmqJC6NTe"
   },
   "outputs": [],
   "source": [
    "env_name = \"Breakout-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOu0PoXz6NTg"
   },
   "source": [
    "`env` -- класс той самой среды, которую мы запускаем\n",
    "\n",
    "Посмотрим, что мы можем извлекать из этого класса:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZrrANgJc6NTg"
   },
   "source": [
    "### пример запуска тестовой среды от Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlYRc6IG6NTh"
   },
   "source": [
    "Запустим код ниже.\n",
    "\n",
    "Процесс начинается с вызова `env.reset()`, который возвращает начальное наблюдение в игре (в данных играх, наблюдение -- это картинка параметы которой описаны в `env.observation_space`).\n",
    "\n",
    "`env.render()` запускает окно с отрисовкой текущего наблюдения\n",
    "\n",
    "Для закрытия окна не забывайте делать `env.close()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "jg2bb1YK6NTi",
    "outputId": "3757d3f5-16d4-478c-ebd6-d946995ef7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode finished after 259 timesteps\n",
      "1\n",
      "Episode finished after 386 timesteps\n",
      "2\n",
      "Episode finished after 165 timesteps\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i_episode in range(20):\n",
    "        observation = env.reset()\n",
    "        print(i_episode)\n",
    "        for t in range(500):\n",
    "            time.sleep(1./30)\n",
    "            env.render()\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1lUzVJB6NTk"
   },
   "source": [
    "По сути, это код работы рандомного агента. Его действия -- это элементы пространства действий игры, причем их выбор этих действий равновероятен \n",
    "\n",
    "Более подробно про среды и работу с ними вы можете прочитать в [документации на официальном сайте OpenAI Gym](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vnj09MVR6NTl"
   },
   "source": [
    "## Инициализация модели\n",
    "\n",
    "За бейзлайн возьмем алгоритм DQN, выход которого равен количеству действий в играх (для игр Atari равен количеству кнопок на джойстике, а именно 18). \n",
    "\n",
    "Для простого старта обучения, вам предоставляется класс DQN-агента (более сложные методы можно найти [тут](https://github.com/keon/deep-q-learning))\n",
    "\n",
    "Ссылки для более подробного изучения:\n",
    "\n",
    "* [статья о DQN на towards data science](https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b)\n",
    "\n",
    "* [фреймворк с RL-моделями на keras](https://github.com/keras-rl/keras-rl)\n",
    "\n",
    "* [Релизация DQN на pytorch](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03JE0mlI6NTm"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 0.05  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=self.state_size))\n",
    "        for _ in range(2):\n",
    "            model.add(Conv2D(8, (3, 3), activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KfwOPm76NTp"
   },
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IPYWtf6Y6NTp"
   },
   "outputs": [],
   "source": [
    "env_name = \"Breakout-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "GRAYSCALE = True\n",
    "observation = env.reset()\n",
    "downsample = 4\n",
    "new_shape = [i // downsample if i > 3 else i for i in observation.shape]\n",
    "if GRAYSCALE:\n",
    "    new_shape[-1] = 1\n",
    "new_shape = tuple(new_shape)\n",
    "\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(new_shape, action_size)\n",
    "##agent.load(\"./pong_2.h5\")\n",
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "def process_state(state, grayscale=GRAYSCALE):\n",
    "    if grayscale:\n",
    "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = cv2.resize(state, new_shape[1::-1])\n",
    "    if grayscale:\n",
    "        state = np.reshape(state, (1,) + state.shape + (1,)) / 255.\n",
    "    else:\n",
    "        state = np.reshape(state, (1,) + state.shape) / 255.\n",
    "    return state\n",
    "\n",
    "EPISODES = 10000\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = process_state(state)\n",
    "    total_reward = 0\n",
    "    for time in tqdm_notebook(range(1000)):\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        next_state = process_state(next_state)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, time: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        \n",
    "    print(\"epoch {}, total_reward = {}\".format(e, total_reward))\n",
    "    # if e % 10 == 0:\n",
    "    #     agent.save(\"./save/cartpole-dqn.h5\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5pRn4pf6NTr"
   },
   "outputs": [],
   "source": [
    "agent.save(\"./Breakout-v0_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DCGRgwQ6NTu"
   },
   "source": [
    "## Тестирование моделей (компьютер vs модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdf8XZkI6NTv"
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = cv2.resize(cv2.cvtColor(state, cv2.COLOR_RGB2GRAY), None, fx=0.5, fy=0.5)\n",
    "state = np.reshape(state, [1, state_size])\n",
    "score = 0\n",
    "\n",
    "for time in range(1000):\n",
    "    env.render()\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, _, _ = env.step(action)\n",
    "    score += reward\n",
    "    next_state = cv2.resize(cv2.cvtColor(next_state, cv2.COLOR_RGB2GRAY), None, fx=0.5, fy=0.5)\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "env.close()\n",
    "print(\"You score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icsI7Iykb7GP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "rl_baseline_v5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
